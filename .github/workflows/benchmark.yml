name: Performance Benchmarks

on:
  # Run benchmarks on schedule (every Sunday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 0'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      compare_branch:
        description: 'Branch to compare against (default: main)'
        required: false
        default: 'main'
  
  # Run on PRs with 'benchmark' label
  pull_request:
    types: [labeled]

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' || 
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'benchmark'))
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,benchmark]"
    
    - name: Run retrieval benchmarks
      run: |
        python benchmarks/benchmark_retrieval.py --output results_retrieval.json
      continue-on-error: true
    
    - name: Run memory benchmarks
      run: |
        python benchmarks/benchmark_memory.py --output results_memory.json
      continue-on-error: true
    
    - name: Compare against baseline
      run: |
        if [ -f benchmarks/baseline_metrics.json ]; then
          python benchmarks/compare_baseline.py \
            --baseline benchmarks/baseline_metrics.json \
            --current results_*.json \
            --output comparison_report.md
        else
          echo "No baseline found. Skipping comparison."
          echo "# Benchmark Results" > comparison_report.md
          echo "" >> comparison_report.md
          echo "Baseline metrics not yet established." >> comparison_report.md
        fi
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          results_*.json
          comparison_report.md
    
    - name: Post PR comment with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const report = fs.readFileSync('comparison_report.md', 'utf8');
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          } catch (error) {
            console.log('Could not post benchmark results:', error);
          }
      continue-on-error: true
    
    - name: Check for performance regressions
      run: |
        if [ -f comparison_report.md ]; then
          # Check if there are any significant regressions (>10% slower)
          if grep -q "❌" comparison_report.md; then
            echo "⚠️  Performance regressions detected!"
            cat comparison_report.md
            exit 1
          else
            echo "✅ No significant performance regressions"
          fi
        fi
      continue-on-error: true

  benchmark-matrix:
    name: Benchmark on Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,benchmark]"
    
    - name: Run quick benchmarks
      run: |
        python benchmarks/benchmark_retrieval.py --quick --output results_py${{ matrix.python-version }}.json
      continue-on-error: true
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-py${{ matrix.python-version }}
        path: results_py${{ matrix.python-version }}.json

  update-baseline:
    name: Update Baseline Metrics
    runs-on: ubuntu-latest
    needs: benchmark
    if: |
      github.event_name == 'schedule' &&
      github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
    
    - name: Update baseline
      run: |
        # Merge current results into baseline
        if [ -f results_retrieval.json ] && [ -f results_memory.json ]; then
          python benchmarks/update_baseline.py \
            --results results_*.json \
            --baseline benchmarks/baseline_metrics.json \
            --output benchmarks/baseline_metrics.json
        fi
      continue-on-error: true
    
    - name: Commit updated baseline
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        if [ -n "$(git status --porcelain benchmarks/baseline_metrics.json)" ]; then
          git add benchmarks/baseline_metrics.json
          git commit -m "chore: update performance baseline metrics [skip ci]"
          git push
        else
          echo "No changes to baseline metrics"
        fi
      continue-on-error: true

  visualize:
    name: Generate Performance Visualizations
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[benchmark]"
    
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
    
    - name: Generate visualizations
      run: |
        python benchmarks/visualize.py \
          --input results_*.json \
          --output benchmark_charts.png
      continue-on-error: true
    
    - name: Upload charts
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-charts
        path: benchmark_charts.png
      if: success()
